# -*- coding: utf-8 -*-
"""drink_generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OY95YaZNLasDi8gjQduaisJf16Bmflm2
"""

from __future__ import unicode_literals, print_function, division
import torch 
import torch.nn as nn
from io import open
import numpy
import glob
import os
import unicodedata
import string
import random
from gensim.models import Word2Vec

ingredients = open('ingredients.txt').readlines()

for i, ing in enumerate(ingredients):
  ingredients[i] = ing[:-1].split(", ")

for ing_list in ingredients:
  for i, ing in enumerate(ing_list):
    if "juice of a " in ing:
      ing_list[i] = ing[12:] + " juice"
    elif "juice of" in ing:
      ing_list[i] = ing[8:] + " juice"
    if ing_list[i][0] == " ":
      ing_list[i] = ing_list[i][1:]
    if "mr. boston " in ing_list[i]:
      ing_list[i] = ing_list[i][10]

all_ingredients = list(set([j for i in ingredients for j in i]))
num_ingredients = len(all_ingredients)
n_ingredients = num_ingredients + 1

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(RNN, self).__init__()
    self.hidden_size = hidden_size

    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
    self.i2o = nn.Linear(input_size + hidden_size, output_size)
    self.o2o = nn.Linear(hidden_size + output_size, output_size)
    self.dropout = nn.Dropout(0.1)
    self.softmax = nn.LogSoftmax(dim=1)

  def forward(self, input, hidden):
    combined = torch.cat((input, hidden), 1)
    hidden = self.i2h(combined)
    output = self.i2o(combined)
    output_combined = torch.cat((hidden, output), 1)
    output = self.o2o(output_combined)
    output = self.dropout(output)
    output = self.softmax(output)
    return output, hidden

  def initHidden(self):
    return torch.zeros(1, self.hidden_size)


def randomChoice(l):
    return l[random.randint(0, len(l) - 1)]



vector_model = Word2Vec(sentences=ingredients, size=num_ingredients, window=2, min_count=1, workers=4)
vector_model.save("word2vec.model")



EOS = torch.from_numpy(numpy.append(numpy.zeros(100), [1]))

def inputTensor(line):
    tensor = torch.zeros(len(line), 1, n_ingredients)
    for li in range(len(line)):
        ingredient = line[li]
        padded = torch.from_numpy(numpy.append(vector_model.wv[ingredient], [0]))
        tensor[li][0] = padded
    return tensor

def targetTensor(line):
  letter_indexes = [all_ingredients.index(line[li]) for li in range(1, len(line))]
  letter_indexes.append(n_ingredients - 1) # EOS
  return torch.LongTensor(letter_indexes)

criterion = nn.NLLLoss()

learning_rate = 0.0005

def model_train(input_line_tensor, target_line_tensor):
  target_line_tensor.unsqueeze_(-1)
  hidden = rnn.initHidden()

  rnn.zero_grad()
  loss = 0

  for i in range(input_line_tensor.size(0)):
    output, hidden = rnn(input_line_tensor[i], hidden)
    l = criterion(output, target_line_tensor[i])
    loss += l

  loss.backward()

  for p in rnn.parameters():
    p.data.add_(p.grad.data, alpha=-learning_rate)
  
  return output, loss.item() / input_line_tensor.size(0)

import time
import math

def timeSince(since):
    now = time.time()
    s = now - since
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

def getTraining():
  example = randomChoice(ingredients)
  #print(example)
  input_line_tensor = inputTensor(example)
  target_line_tensor = targetTensor(example)
  return input_line_tensor, target_line_tensor

rnn = RNN(n_ingredients, 128, n_ingredients)

n_iters = 20000
print_every = 2000
plot_every = 500
all_losses = []
total_loss = 0 # Reset every plot_every iters

start = time.time()

for iter in range(1, n_iters + 1):
  input, target = getTraining()
  #print(target)
  output, loss = model_train(input, target)
  total_loss += loss

  if iter % print_every == 0:
      print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))

  if iter % plot_every == 0:
      all_losses.append(total_loss / plot_every)
      total_loss = 0

max_length = 6

def sample(start_ingredient="vodka"):
  with torch.no_grad():
    input = inputTensor([start_ingredient])
    hidden = rnn.initHidden()

    output_ingredients = [start_ingredient]
    for i in range(max_length):
      output, hidden = rnn(input[0], hidden)
      topv, topi = output.topk(1)
      topi = topi[0][0]
      if topi == n_ingredients - 1:
          break
      else:
          ingredient = all_ingredients[topi]
          output_ingredients.append(ingredient)
      input = inputTensor([ingredient])
    
    return output_ingredients